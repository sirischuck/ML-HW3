{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUdciWKdOsoHLa9/fF9Zj7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sirischuck/ML-HW3/blob/main/Ass3_Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorboard\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gt_JYsUGFMJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfcd487-95b7-435a-a176-c45e78cc0686"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gmNrBGcXEsMg"
      },
      "outputs": [],
      "source": [
        "(x_train,y_train), (x_test,y_test)= keras.datasets.cifar10.load_data()\n",
        "x_train=(x_train/255.0)\n",
        "x_test=x_test/255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize=128\n",
        "learningRate=.001\n",
        "print(batchSize, learningRate)\n",
        "\n",
        "LeNet = tf.keras.models.Sequential()\n",
        "LeNet.add(keras.layers.Conv2D(6, (5,5), activation='relu'))\n",
        "LeNet.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "LeNet.add(keras.layers.Conv2D(16,(5,5), activation='relu'))\n",
        "LeNet.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "LeNet.add(keras.layers.Conv2D(120,(5,5), activation='relu'))\n",
        "\n",
        "LeNet.add(keras.layers.Flatten())\n",
        "LeNet.add(keras.layers.Dense(84))\n",
        "LeNet.add(keras.layers.Activation('relu'))\n",
        "LeNet.add(tf.keras.layers.Dense(10))\n",
        "LeNet.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "opt= keras.optimizers.Adam(learning_rate=learningRate)\n",
        "LeNet.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "LeNet.build(input_shape=(1,32,32,3))\n",
        "LeNet.summary()\n",
        "\n",
        "LeNet.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batchSize,\n",
        "    epochs=25)\n",
        "\n",
        "\n",
        "\n",
        "score=LeNet.evaluate(x_test,y_test)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLBg-yccMQn-",
        "outputId": "e01176c5-8e86-48fd-b1e7-d450ea6a6258"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128 0.001\n",
            "Epoch 1/25\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.7399 - accuracy: 0.3632\n",
            "Epoch 2/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.4555 - accuracy: 0.4743\n",
            "Epoch 3/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.3597 - accuracy: 0.5126\n",
            "Epoch 4/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.2975 - accuracy: 0.5360\n",
            "Epoch 5/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.2454 - accuracy: 0.5564\n",
            "Epoch 6/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.2020 - accuracy: 0.5748\n",
            "Epoch 7/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.1668 - accuracy: 0.5851\n",
            "Epoch 8/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.1341 - accuracy: 0.5985\n",
            "Epoch 9/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.1086 - accuracy: 0.6077\n",
            "Epoch 10/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.0794 - accuracy: 0.6167\n",
            "Epoch 11/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.0604 - accuracy: 0.6244\n",
            "Epoch 12/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.0375 - accuracy: 0.6324\n",
            "Epoch 13/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.0144 - accuracy: 0.6429\n",
            "Epoch 14/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.9930 - accuracy: 0.6483\n",
            "Epoch 15/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.9728 - accuracy: 0.6576\n",
            "Epoch 16/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.9547 - accuracy: 0.6627\n",
            "Epoch 17/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.9419 - accuracy: 0.6657\n",
            "Epoch 18/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.9243 - accuracy: 0.6736\n",
            "Epoch 19/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.9075 - accuracy: 0.6810\n",
            "Epoch 20/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.9014 - accuracy: 0.6823\n",
            "Epoch 21/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.8756 - accuracy: 0.6935\n",
            "Epoch 22/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.8620 - accuracy: 0.6974\n",
            "Epoch 23/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.8541 - accuracy: 0.6980\n",
            "Epoch 24/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.8364 - accuracy: 0.7054\n",
            "Epoch 25/25\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.8244 - accuracy: 0.7101\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.1181 - accuracy: 0.6169\n",
            "[1.1181057691574097, 0.6169000267982483]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize=128\n",
        "learningRate=.001\n",
        "MLP = tf.keras.models.Sequential()\n",
        "MLP.add(keras.layers.Flatten())\n",
        "\n",
        "MLP.add(keras.layers.Dense(6, activation='relu'))\n",
        "MLP.add(keras.layers.Dense(16, activation='relu'))\n",
        "MLP.add(keras.layers.Dense(120, activation='relu'))\n",
        "\n",
        "MLP.add(keras.layers.Dense(84))\n",
        "MLP.add(keras.layers.Activation('relu'))\n",
        "MLP.add(tf.keras.layers.Dense(10))\n",
        "MLP.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "opt= keras.optimizers.Adam(learning_rate=learningRate)\n",
        "MLP.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "MLP.build(input_shape=(1,32,32,3))\n",
        "MLP.summary()\n",
        "\n",
        "MLP.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batchSize,\n",
        "    epochs=25)\n",
        "\n",
        "score=MLP.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dztms_gjFWgN",
        "outputId": "4ec2b13f-55b8-49b9-eeaa-e2a1a2772841"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (1, 3072)                 0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (1, 6)                    18438     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (1, 16)                   112       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (1, 120)                  2040      \n",
            "                                                                 \n",
            " dense_12 (Dense)            (1, 84)                   10164     \n",
            "                                                                 \n",
            " activation_6 (Activation)   (1, 84)                   0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (1, 10)                   850       \n",
            "                                                                 \n",
            " activation_7 (Activation)   (1, 10)                   0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,604\n",
            "Trainable params: 31,604\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "391/391 [==============================] - 2s 3ms/step - loss: 2.1510 - accuracy: 0.1686\n",
            "Epoch 2/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0513 - accuracy: 0.1991\n",
            "Epoch 3/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0409 - accuracy: 0.1997\n",
            "Epoch 4/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0340 - accuracy: 0.2038\n",
            "Epoch 5/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0336 - accuracy: 0.2024\n",
            "Epoch 6/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0281 - accuracy: 0.2047\n",
            "Epoch 7/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0303 - accuracy: 0.2023\n",
            "Epoch 8/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0269 - accuracy: 0.2017\n",
            "Epoch 9/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0255 - accuracy: 0.2084\n",
            "Epoch 10/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0258 - accuracy: 0.2063\n",
            "Epoch 11/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0235 - accuracy: 0.2059\n",
            "Epoch 12/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0258 - accuracy: 0.2065\n",
            "Epoch 13/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0257 - accuracy: 0.2050\n",
            "Epoch 14/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0239 - accuracy: 0.2048\n",
            "Epoch 15/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0232 - accuracy: 0.2070\n",
            "Epoch 16/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0193 - accuracy: 0.2078\n",
            "Epoch 17/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0224 - accuracy: 0.2082\n",
            "Epoch 18/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0197 - accuracy: 0.2072\n",
            "Epoch 19/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0218 - accuracy: 0.2077\n",
            "Epoch 20/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0211 - accuracy: 0.2091\n",
            "Epoch 21/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0199 - accuracy: 0.2070\n",
            "Epoch 22/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0191 - accuracy: 0.2091\n",
            "Epoch 23/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0183 - accuracy: 0.2094\n",
            "Epoch 24/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0181 - accuracy: 0.2065\n",
            "Epoch 25/25\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.0183 - accuracy: 0.2104\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 2.0140 - accuracy: 0.2054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Learning rate had a huge effect on the training process. In general, having a higher learning rate makes it very likely that the model will get stuck at random guessing, because each stochastic step will move the model too far to be useful. If the model does converge with a high learning rate, the accuracy will increase quickly in the first few epochs, but won't converge to a very high accuracy. A low learning rate has the oposite problems, with the added complication that sometimes it will overfit to the training data with a very low test accuracy. My best accuracy was with a learning rate of .001 with a test accuracy of 62.19\n",
        "\n",
        "2. Batch size had a similar effect to learning rate, where with a low batch size, the model will take more steps and converge faster at first. However, with a very low batch size, you risk only gathering data from a few samples and not gathering a robust set of features at each step. My best batch size was 32 with a test accuracy of \n",
        "\n",
        "3. For the last questions I already wrote a program to test all the batch sizes and learning rates that I was interested in. This means that my best test accuracy was simply with a batch size of 128 and learning rate of .001 with an accuracy of 62.19%.\n",
        "\n",
        "4. The Feed foreward network had terrible performance compared to the CNN. The CNN had a test accuracy of 62.19% whereas the MLP had an accuracy of 20.13. The MLP did have a much lower number of trainable parameters with 31,604 as opposed to the LeNet's 62,006.\n"
      ],
      "metadata": {
        "id": "9Cg_AbGz_27e"
      }
    }
  ]
}